# -*- coding: utf-8 -*-
"""Airline Sentiment Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/jain-abhinav02/Sentiment-Classfication/blob/master/Airline_Sentiment_Classification.ipynb
"""

import numpy
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense,LSTM,GRU,Dropout,BatchNormalization
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from keras.layers import Embedding
from sklearn.preprocessing import OneHotEncoder
import numpy as np
import os
from keras.initializers import Constant

# Mount the drive
from google.colab import drive
drive.mount('/content/drive')

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove*.zip

!ls
!pwd

# processing the glove embedding txt file
GLOVE_DIR = ''
embeddings_index = {}
with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, 'f', sep=' ')
        embeddings_index[word] = coefs

print('Found %s word vectors.' % len(embeddings_index))

# import dataset into a pandas dataframe
tweets1 = pd.read_csv("drive/My Drive/Datasets/Tweets.csv")

# choosing a subset of the dataset to test models quickly
tweets = tweets1.iloc[:,:]
print(type(tweets))

# a glimpse of the dataset
print(tweets.shape)
tweets.head()

# Airline wise tweets count
air_wise_tweets = tweets['airline'].value_counts()
print(air_wise_tweets)
print()
print(air_wise_tweets.plot.bar())

# Overall count in each of the sentiment classes 
# positive, negative, neutral
print(tweets.airline_sentiment.value_counts())

# Sentiment airline wise
air_wise_sent = tweets.groupby('airline').airline_sentiment.value_counts().unstack().fillna(0)
print(air_wise_sent)
print(air_wise_sent.plot.bar(stacked=False))

# One hot encode the output labels ( positive, negative , neutral)
y = tweets.iloc[:,1:2].values
encoder = OneHotEncoder()
y = encoder.fit_transform(y).toarray()

# Extract the tweets column and remove the prefix @airline_name
x = tweets['text']
x= x.str.extract('\w\s([\s\S]*)',expand=True)
x1 = x.iloc[:,0].values

# Find the maximum length of any review
print('Maximum review length: {}'.format(len(max(x1, key=len))))

maxlen=150
embedding_size = 100
MAX_NUM_WORDS = 20000

# Tokenize the text reviews to integer arrays
tk = Tokenizer(lower = True)
tk.fit_on_texts(x1)
x_seq = tk.texts_to_sequences(x1)
x_pad = pad_sequences(x_seq,maxlen=maxlen)

vocab_size = len(tk.word_counts.keys())+1

# Counting the number of unique words in all reviews
word_index = tk.word_index
print('Found %s unique tokens.' % len(word_index))

# preparing embedding matrix
num_words = min(MAX_NUM_WORDS, len(word_index)) + 1
embedding_matrix = np.zeros((num_words, embedding_size))
for word, i in word_index.items():
    if i > MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

# splitting training and test data
x_train,x_test,y_train,y_test = train_test_split(x_pad,y,test_size=0.25,random_state=1)

# splitting training and validation data
valid_size = 64
x_train1 = x_train[valid_size:]
y_train1 = y_train[valid_size:]
x_val = x_train[:valid_size]
y_val = y_train[:valid_size]

print(x_train.shape)

# create a RNN model
model = Sequential()
#model.add(Embedding(vocab_size,embedding_size,embeddings_initializer=Constant(embedding_matrix),input_length = maxlen,trainable=False))
#model.add(LSTM(100,return_sequences = True))
model.add(Embedding(vocab_size,embedding_size,input_length=maxlen))
model.add(LSTM(256))

model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Dense(512,activation='relu'))

model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Dense(3,activation = 'softmax'))

# review the model
model.compile(loss= 'categorical_crossentropy',optimizer = 'adam', metrics = ['accuracy'])
print(model.summary())

# begin training on dataset
batch_size = 128
num_epochs = 5
model.fit(x_train1,y_train1,validation_data= (x_val,y_val),batch_size = batch_size, epochs= num_epochs)

# check accuracy on test data
scores = model.evaluate(x_test,y_test,verbose=0)
print("accuracy:",str(scores[1]))

u=8
print(tk.sequences_to_texts(x_test[u:u+1]))
print(model.predict(x_test[u:u+1]))